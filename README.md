# VL
V+L Research

# Paper List

## Current Work References
1. **Target: SOHO**
2. Vokenization (English Wikipedia)
3. BUTD
4. V+L Contrastive Learning Series: CLIP
5. V+L VQ Series: SOHO, DALL-E(1,2)

## Multimodal Pretraining
1. **SOHO** 
2. **X-VLM** (SOTA)
3. GLIP v1,**2**
4. CLIP (+CLIP-event and other follow-ups)
5. SimVLM
6. UNIMO 
7. VirTex
8. ALIGN 
9. Oscar 
10. Florence
11. CoOp (Learning to Prompt VLM)
12. BriVL (WenLan2.0)
13. Multimodal Few-Shot Learning with Frozen Language Models 

## Multilingual Pretraining 
1. Turing-URL v2 (InfoXLM): MLM+TLM+XLCo
2. Turing-ULR v5 (XLM-E(LECTRA)): Corrective Language Modeling MRTD+TRTD (SOTA)
  
## Vision Latent Space Modeling
1. VQ-VAE (1, **2 for hierarchy**)
	* PeCo, Discrete Representation Strengthen ViT Robustness
	* DALL-E (1, **2 for hierarchy**) (Vision GPT-3, using dVAE+Transformer+CLIP, Logit-Laplace), NUWA
	* VQ-GAN, ViT VQ-GAN
2. Style-GAN 
	* Style-CLIP
4. MiCE
6. PCL (Prototypical Contrastive Learning)
	* FUDA (Prototypical Cross-domain SSL for Few-shot Unsupervised Domain Adaptation) 
	* **HCSC (Hierarchical Contrastive Selective Coding)**
 
 ## Vision Self Supervise 
1. Contrastive:
	1. **MoCo** (1, 2, 3)
	2. SimCLR (1, 2), NNCLR
	3. BYOL
	4. SwAV
	5. BarlowTwins
	6. SimSiam
	7. PCL
	8. AdCo
	9. **HCSC**
2. Auto Regression / BERT like Mask Image Modeling
	1. SimMIM
	2. DINO
	3. BEiT
	4. MAE
	5. **PeCo**
	6. **CAE**: Representation Learning & Pre-text Task separation
