# VL
V+L Research

# Paper List
  
## Multimodal Pretraining
	1. **SOHO** 
	2. **X-VLM** (SOTA)
	3. CLIP (+CLIP-event and other follow-ups)
	4. SimVLM
	5. UNIMO 
	6. VirTex
	7. ALIGN 
	8. Oscar 
	9. Florence
	10. CoOp (Learning to Prompt VLM)
	11. BriVL (WenLan2.0)
	12. Multimodal Few-Shot Learning with Frozen Language Models 

## Multilingual Pretraining
	1. Turing-URL v2 (InfoXLM): MLM+TLM+XLCo
	2. Turing-ULR v5 (XLM-E(LECTRA)): Corrective Language Modeling MRTD+TRTD (SOTA)
  
## Vision Latent Space Modeling
	1. VQ-VAE (1, 2) 
	-> PeCo, Discrete Representation Strengthen ViT Robustness 
  	-> DALL-E (Vision GPT-3, using dVAE+Transformer+CLIP, Logit-Laplace), NUWA
  	-> VQ-GAN, ViT VQ-GAN
	2. Style-GAN -> Style-CLIP
	3. MiCE
	4. PCL (Prototypical Contrastive Learning)
  	-> FUDA (Prototypical Cross-domain SSL for Few-shot Unsupervised Domain Adaptation) 
  	-> HCSC (Hierarchical Contrastive Selective Coding)
 
 ## Vision Self Supervise 
	1. Contrastive:
		a. MoCo (1, 2, 3)
		b. SimCLR (1, 2), NNCLR
		c. BYOL
		d. SwAV
		e. BarlowTwins
		f. SimSiam
		g. PCL
    		h. AdCo
	2. Auto Regression / BERT like Mask Image Modeling
		a. SimMIM
		b. DINO
		c. BEiT
		d. MAE
		e. PeCo
		f. CAE: Representation Learning & Pre-text Task separation
