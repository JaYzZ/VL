# VL
V+L Research

# Paper List

## Current Work References
1. **Target: SOHO**
2. Vokenization (English Wikipedia)
3. BUTD
4. V+L Contrastive Learning Series: CLIP
5. V+L VQ Series: SOHO, DALL-E(1,2)

## Multimodal Pretraining
1. **SOHO** 
2. **ERNIE-ViL**
3. **X-VLM** (SOTA)
4. GLIP v1,**2**
5. CLIP (+CLIP-event and other follow-ups)
6. SimVLM
7. UNIMO 
8. VirTex
9. ALIGN 
10. Oscar 
11. Florence
12. CoOp (Learning to Prompt VLM)
13. BriVL (WenLan2.0)
14. Multimodal Few-Shot Learning with Frozen Language Models 
15. VLMo
16. Parti

## Multilingual Pretraining 
1. Turing-URL v2 (InfoXLM): MLM+TLM+XLCo
2. Turing-ULR v5 (XLM-E(LECTRA)): Corrective Language Modeling MRTD+TRTD (SOTA)
  
## Vision Latent Space Modeling
1. VQ-VAE (1, **2 for hierarchy**)
	* PeCo, Discrete Representation Strengthen ViT Robustness
	* DALL-E (1, **2 for hierarchy**) (Vision GPT-3, using dVAE+Transformer+CLIP, Logit-Laplace), NUWA
	* VQ-GAN, ViT VQ-GAN
2. Style-GAN 
	* Style-CLIP
4. MiCE
6. PCL (Prototypical Contrastive Learning)
	* FUDA (Prototypical Cross-domain SSL for Few-shot Unsupervised Domain Adaptation) 
	* **HCSC (Hierarchical Contrastive Selective Coding)**
 
 ## Vision Self Supervise 
1. Contrastive:
	1. **MoCo** (1, 2, 3)
	2. SimCLR (1, 2), NNCLR
	3. BYOL
	4. SwAV
	5. BarlowTwins
	6. SimSiam
	7. PCL
	8. AdCo
	9. **HCSC**
2. Auto Regression / BERT like Mask Image Modeling
	1. SimMIM
	2. DINO
	3. BEiT v1-3
	4. MAE
	5. **PeCo**
	6. **CAE**: Representation Learning & Pre-text Task separation
	7. MILAN

 ## Miscellaneous Reference
1. kMaxTransformer
2. GiT
